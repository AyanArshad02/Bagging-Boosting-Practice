{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection & Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>ProdTaken</th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  ProdTaken  ...  Designation MonthlyIncome\n",
       "0      200000          1  ...      Manager       20993.0\n",
       "1      200001          0  ...      Manager       20130.0\n",
       "\n",
       "[2 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Travel.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "#### Handling Missing Values\n",
    "1. Handling Missing Values\n",
    "2. Handling Duplicates\n",
    "3. Check Data Type\n",
    "4. Understand the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerID                    0\n",
       "ProdTaken                     0\n",
       "Age                         226\n",
       "TypeofContact                25\n",
       "CityTier                      0\n",
       "DurationOfPitch             251\n",
       "Occupation                    0\n",
       "Gender                        0\n",
       "NumberOfPersonVisiting        0\n",
       "NumberOfFollowups            45\n",
       "ProductPitched                0\n",
       "PreferredPropertyStar        26\n",
       "MaritalStatus                 0\n",
       "NumberOfTrips               140\n",
       "Passport                      0\n",
       "PitchSatisfactionScore        0\n",
       "OwnCar                        0\n",
       "NumberOfChildrenVisiting     66\n",
       "Designation                   0\n",
       "MonthlyIncome               233\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender\n",
       "Male       2916\n",
       "Female     1817\n",
       "Fe Male     155\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].replace('Fe Male', 'Female') # Replacing \"Fe Male\" with \"Female\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender\n",
       "Male      2916\n",
       "Female    1972\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaritalStatus\n",
       "Married      2340\n",
       "Divorced      950\n",
       "Single        916\n",
       "Unmarried     682\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MaritalStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaritalStatus\n",
       "Married      2340\n",
       "Unmarried    1598\n",
       "Divorced      950\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MaritalStatus'] = df['MaritalStatus'].replace('Single','Unmarried')\n",
    "df['MaritalStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 4.62357 % missing values\n",
      "TypeofContact 0.51146 % missing values\n",
      "DurationOfPitch 5.13502 % missing values\n",
      "NumberOfFollowups 0.92062 % missing values\n",
      "PreferredPropertyStar 0.53191 % missing values\n",
      "NumberOfTrips 2.86416 % missing values\n",
      "NumberOfChildrenVisiting 1.35025 % missing values\n",
      "MonthlyIncome 4.76678 % missing values\n"
     ]
    }
   ],
   "source": [
    "# Checking Missing Values\n",
    "features_with_nan = [features for features in df.columns if df[features].isnull().sum()>=1]\n",
    "for feature in features_with_nan:\n",
    "    print(feature, np.round(df[feature].isnull().mean()*100,5), '% missing values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4662.000000</td>\n",
       "      <td>4637.000000</td>\n",
       "      <td>4843.000000</td>\n",
       "      <td>4862.000000</td>\n",
       "      <td>4748.000000</td>\n",
       "      <td>4822.000000</td>\n",
       "      <td>4655.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.622265</td>\n",
       "      <td>15.490835</td>\n",
       "      <td>3.708445</td>\n",
       "      <td>3.581037</td>\n",
       "      <td>3.236521</td>\n",
       "      <td>1.187267</td>\n",
       "      <td>23619.853491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.316387</td>\n",
       "      <td>8.519643</td>\n",
       "      <td>1.002509</td>\n",
       "      <td>0.798009</td>\n",
       "      <td>1.849019</td>\n",
       "      <td>0.857861</td>\n",
       "      <td>5380.698361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22347.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>98678.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age  DurationOfPitch  ...  NumberOfChildrenVisiting  MonthlyIncome\n",
       "count  4662.000000      4637.000000  ...               4822.000000    4655.000000\n",
       "mean     37.622265        15.490835  ...                  1.187267   23619.853491\n",
       "std       9.316387         8.519643  ...                  0.857861    5380.698361\n",
       "min      18.000000         5.000000  ...                  0.000000    1000.000000\n",
       "25%      31.000000         9.000000  ...                  1.000000   20346.000000\n",
       "50%      36.000000        13.000000  ...                  1.000000   22347.000000\n",
       "75%      44.000000        20.000000  ...                  2.000000   25571.000000\n",
       "max      61.000000       127.000000  ...                  3.000000   98678.000000\n",
       "\n",
       "[8 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[features_with_nan].select_dtypes(exclude = 'object').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Null Values\n",
    "1. Impute Median Value for Age\n",
    "2. Impute Mode for TypeofContact\n",
    "3. Impute Median for DurationOfPitch\n",
    "4. Impute Mode for NumberOfFollowups as it's Discrete Feature\n",
    "5. Impute Mode for PreferredPropertyStar\n",
    "6. Impute Median for NumberOfTrips\n",
    "7. Impute Mode for NumberOfChildrenVisiting\n",
    "8. Impute Median for MonthlyIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "\n",
    "# TypeofContact\n",
    "df['TypeofContact'].fillna(df['TypeofContact'].mode()[0], inplace=True)\n",
    "\n",
    "# DurationOfPitch\n",
    "df['DurationOfPitch'].fillna(df['DurationOfPitch'].median(), inplace=True)\n",
    "\n",
    "# NumberOfFollowups\n",
    "df['NumberOfFollowups'].fillna(df['NumberOfFollowups'].mode()[0], inplace=True)\n",
    "\n",
    "# PreferredPropertyStar\n",
    "df['PreferredPropertyStar'].fillna(df['PreferredPropertyStar'].mode()[0], inplace = True)\n",
    "\n",
    "# NumberOfTrips\n",
    "df['NumberOfTrips'].fillna(df['NumberOfTrips'].median(), inplace = True)\n",
    "\n",
    "# NumberOfChildrenVisiting\n",
    "df['NumberOfChildrenVisiting'].fillna(df['NumberOfChildrenVisiting'].mode()[0], inplace=True)\n",
    "\n",
    "# MonthlyIncome\n",
    "df['MonthlyIncome'].fillna(df['MonthlyIncome'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerID                  0\n",
       "ProdTaken                   0\n",
       "Age                         0\n",
       "TypeofContact               0\n",
       "CityTier                    0\n",
       "DurationOfPitch             0\n",
       "Occupation                  0\n",
       "Gender                      0\n",
       "NumberOfPersonVisiting      0\n",
       "NumberOfFollowups           0\n",
       "ProductPitched              0\n",
       "PreferredPropertyStar       0\n",
       "MaritalStatus               0\n",
       "NumberOfTrips               0\n",
       "Passport                    0\n",
       "PitchSatisfactionScore      0\n",
       "OwnCar                      0\n",
       "NumberOfChildrenVisiting    0\n",
       "Designation                 0\n",
       "MonthlyIncome               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProdTaken</th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Free Lancer</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>17090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>17909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>18468.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProdTaken   Age  ... Designation  MonthlyIncome\n",
       "0          1  41.0  ...     Manager        20993.0\n",
       "1          0  49.0  ...     Manager        20130.0\n",
       "2          1  37.0  ...   Executive        17090.0\n",
       "3          0  33.0  ...   Executive        17909.0\n",
       "4          0  36.0  ...   Executive        18468.0\n",
       "\n",
       "[5 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('CustomerID', axis = 1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalVisiting'] = df['NumberOfChildrenVisiting'] + df['NumberOfPersonVisiting']\n",
    "df.drop(columns=['NumberOfChildrenVisiting', 'NumberOfPersonVisiting'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numerical features is : 13\n"
     ]
    }
   ],
   "source": [
    "# Selecting all numeric features and storing it in variable named num_features\n",
    "num_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
    "print(\"Number of numerical features is :\", len(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical feature is 6\n"
     ]
    }
   ],
   "source": [
    "# Selecting all categorical features and storing it in variable named cat_features\n",
    "cat_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
    "print(f\"Number of categorical feature is {len(cat_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discrete features is 9\n"
     ]
    }
   ],
   "source": [
    "# Selecting all discrete features and storing it in discrete_features\n",
    "discrete_features = [feature for feature in num_features if len(df[feature].unique()) <= 25]\n",
    "print(f\"Number of discrete features is {len(discrete_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of continous features is 4\n"
     ]
    }
   ],
   "source": [
    "# Selecting all continous features and storing in in continous_features\n",
    "continous_features = [feature for feature in num_features if feature not in discrete_features]\n",
    "print(f\"Number of continous features is {len(continous_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('ProdTaken', axis = 1)\n",
    "y = df['ProdTaken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3910, 18), (978, 18))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column transformer with 3 types of transformer\n",
    "cat_features = X.select_dtypes(include='object').columns\n",
    "num_features = X.select_dtypes(exclude='object').columns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying transformation in training dataset \n",
    "# NOTE : in training dateset USE fit_transform() & in test dataset USE transform() to avoid data leakage\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score,f1_score,roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 1.0000\n",
      "- F1 score : 1.0000\n",
      "- Precision : 1.0000\n",
      "- Recall : 1.0000\n",
      "- ROC AUC Score : 1.0000\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.9121\n",
      "- F1 score : 0.9035\n",
      "- Precision : 0.9487\n",
      "- Recall : 0.5812\n",
      "- ROC AUC Score : 0.7868\n",
      "===================================\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 1.0000\n",
      "- F1 score : 1.0000\n",
      "- Precision : 1.0000\n",
      "- Recall : 1.0000\n",
      "- ROC AUC Score : 1.0000\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.9008\n",
      "- F1 score : 0.8995\n",
      "- Precision : 0.7640\n",
      "- Recall : 0.7120\n",
      "- ROC AUC Score : 0.8293\n",
      "===================================\n",
      "\n",
      "\n",
      "AdaBoost Classifier\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 0.8691\n",
      "- F1 score : 0.8518\n",
      "- Precision : 0.7863\n",
      "- Recall : 0.4088\n",
      "- ROC AUC Score : 0.6917\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.8446\n",
      "- F1 score : 0.8230\n",
      "- Precision : 0.7053\n",
      "- Recall : 0.3508\n",
      "- ROC AUC Score : 0.6576\n",
      "===================================\n",
      "\n",
      "\n",
      "Gradient Boost Classifier\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 0.8923\n",
      "- F1 score : 0.8796\n",
      "- Precision : 0.8793\n",
      "- Recall : 0.4897\n",
      "- ROC AUC Score : 0.7372\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.8640\n",
      "- F1 score : 0.8459\n",
      "- Precision : 0.7959\n",
      "- Recall : 0.4084\n",
      "- ROC AUC Score : 0.6915\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is an efficient way of training models\n",
    "models = {\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"Decision Tree\":DecisionTreeClassifier(),\n",
    "    \"AdaBoost Classifier\" : AdaBoostClassifier(),\n",
    "    \"Gradient Boost Classifier\":GradientBoostingClassifier()\n",
    "} # we can add many algorithms here to see which is performing better\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train,y_train) # Model Training\n",
    "\n",
    "    # Making predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Training set performance\n",
    "    model_train_accuracy = accuracy_score(y_train,y_train_pred)\n",
    "    model_train_f1 = f1_score(y_train,y_train_pred,average='weighted')\n",
    "    model_train_precision = precision_score(y_train,y_train_pred)\n",
    "    model_train_recall = recall_score(y_train,y_train_pred)\n",
    "    model_train_roc_auc_score = roc_auc_score(y_train,y_train_pred)\n",
    "\n",
    "    # Test set performance\n",
    "    model_test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "    model_test_f1 = f1_score(y_test,y_test_pred, average='weighted')\n",
    "    model_test_precision = precision_score(y_test,y_test_pred)\n",
    "    model_test_recall = recall_score(y_test,y_test_pred)\n",
    "    model_test_roc_auc_score = roc_auc_score(y_test,y_test_pred)\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Model Performance for training set')\n",
    "    print('- Accuracy : {:.4f}'.format(model_train_accuracy))\n",
    "    print('- F1 score : {:.4f}'.format(model_train_f1))\n",
    "    print('- Precision : {:.4f}'.format(model_train_precision))\n",
    "    print('- Recall : {:.4f}'.format(model_train_recall))\n",
    "    print('- ROC AUC Score : {:.4f}'.format(model_train_roc_auc_score))\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Model Performance for testing set')\n",
    "    print('- Accuracy : {:.4f}'.format(model_test_accuracy))\n",
    "    print('- F1 score : {:.4f}'.format(model_test_f1))\n",
    "    print('- Precision : {:.4f}'.format(model_test_precision))\n",
    "    print('- Recall : {:.4f}'.format(model_test_recall))\n",
    "    print('- ROC AUC Score : {:.4f}'.format(model_test_roc_auc_score))\n",
    "\n",
    "\n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can clearly see by using random forest instead of decision tree , our accuracy for test data has increased because decision tree leads to overfitting i.e. low bias and high variance and random forest helps us in making generalized model by converting high variance into low variance. Therefore, by using Random Forest we get generalized model with high train and test accuracy i.e. low bias and low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "rf_params = {\n",
    "    \"max_depth\":[5,8,15,None,10],\n",
    "    \"max_features\":[5,7,\"auto\",8],\n",
    "    \"min_samples_split\":[2,8,15,20],\n",
    "    \"n_estimators\":[100,200,500,1000]         \n",
    "             }\n",
    "rf_params\n",
    "\n",
    "adaboost_params = {\n",
    "    \"n_estimators\":[50,60,70,80,90],\n",
    "    \"algorithm\":['SAMME','SAMME.R']\n",
    "}\n",
    "\n",
    "gboost_params = {\n",
    "    \"loss\":['log_loss','deviance','exponential'],\n",
    "    \"criterion\":['friedman_mse','squared_error','mse'],\n",
    "    \"min_samples_split\":[2,8,15,20],\n",
    "    \"n_estimators\":[100,200,500,1000],\n",
    "    \"max_depth\":[5,8,15,None,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models list for hyperparameter tuning\n",
    "randomcv_models = [\n",
    "    (\"RF\",RandomForestClassifier(),rf_params),\n",
    "    (\"AdaBoost\",AdaBoostClassifier(),adaboost_params),\n",
    "    (\"Gradient Boost\",GradientBoostingClassifier(),gboost_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.5s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.5s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=8, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=8, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=8, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=20, n_estimators=1000; total time=   3.6s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=20, n_estimators=1000; total time=   3.6s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=2, n_estimators=1000; total time=   4.2s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=2, n_estimators=1000; total time=   4.2s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=2, n_estimators=1000; total time=   4.6s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=20, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=20, n_estimators=1000; total time=   3.7s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=20, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=20, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.1s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=8, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=8, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=20, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=20, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=20, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=8, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.3s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=8, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=8, n_estimators=500; total time=   2.2s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=8, n_estimators=500; total time=   2.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=2, n_estimators=500; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=15, n_estimators=500; total time=   2.0s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=8, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=2, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=15, n_estimators=500; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=2, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=15, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=15, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=15, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=8, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=8, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=15, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=15, n_estimators=500; total time=   1.8s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=15, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=500; total time=   1.3s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=500; total time=   1.5s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=15, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=20, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=2, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=20, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=2, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=20, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=1000; total time=   4.1s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=1000; total time=   4.1s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=1000; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=2, n_estimators=1000; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=15, n_estimators=1000; total time=   4.0s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=8, n_estimators=500; total time=   2.0s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=2, n_estimators=1000; total time=   4.4s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=8, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=2, n_estimators=1000; total time=   4.6s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=15, n_estimators=1000; total time=   4.6s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=8, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=8, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=8, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=8, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=200; total time=   0.9s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=15, n_estimators=1000; total time=   4.4s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=2, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=2, n_estimators=1000; total time=   2.6s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=2, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=8, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=8, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=8, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.8s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=15, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=15, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=15, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=8, min_samples_split=8, n_estimators=1000; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=8, min_samples_split=8, n_estimators=1000; total time=   4.4s\n",
      "[CV] END max_depth=None, max_features=8, min_samples_split=8, n_estimators=1000; total time=   4.6s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=15, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=15, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=15, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=20, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=8, max_features=5, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=20, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=20, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=20, n_estimators=200; total time=   0.8s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=15, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=15, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=15, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=8, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=8, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=20, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=8, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=20, n_estimators=500; total time=   1.5s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=20, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=1000; total time=   3.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=20, n_estimators=1000; total time=   2.6s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=8, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=8, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=8, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=20, n_estimators=1000; total time=   2.7s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=20, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=1000; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=1000; total time=   3.7s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=1000; total time=   3.9s\n",
      "[CV] END max_depth=10, max_features=5, min_samples_split=2, n_estimators=500; total time=   1.5s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=8, n_estimators=500; total time=   2.0s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=8, n_estimators=500; total time=   2.2s\n",
      "[CV] END max_depth=10, max_features=8, min_samples_split=8, n_estimators=500; total time=   2.0s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=15, n_estimators=1000; total time=   3.1s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=15, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=15, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=15, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=8, n_estimators=500; total time=   1.0s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=8, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=7, min_samples_split=15, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=8, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=20, n_estimators=1000; total time=   2.7s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=20, n_estimators=1000; total time=   2.9s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=auto, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=20, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=15, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=15, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=5, max_features=5, min_samples_split=15, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, max_features=8, min_samples_split=20, n_estimators=1000; total time=   3.9s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=None, max_features=8, min_samples_split=20, n_estimators=1000; total time=   3.9s\n",
      "[CV] END max_depth=None, max_features=8, min_samples_split=20, n_estimators=1000; total time=   4.1s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=1000; total time=   2.9s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.7s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=500; total time=   1.4s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=15, max_features=auto, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=2, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=8, n_estimators=1000; total time=   3.3s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=8, n_estimators=1000; total time=   3.3s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=15, n_estimators=500; total time=   1.8s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=15, n_estimators=500; total time=   1.8s\n",
      "[CV] END max_depth=8, max_features=8, min_samples_split=8, n_estimators=1000; total time=   3.8s\n",
      "[CV] END max_depth=15, max_features=7, min_samples_split=15, n_estimators=500; total time=   1.8s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=8, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=8, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=15, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=15, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=15, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=8, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=15, n_estimators=1000; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=5, max_features=7, min_samples_split=15, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=15, n_estimators=1000; total time=   2.4s\n",
      "[CV] END max_depth=5, max_features=8, min_samples_split=15, n_estimators=1000; total time=   2.2s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=15, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=15, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=5, min_samples_split=15, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=20, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=20, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=20, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=8, n_estimators=1000; total time=   4.2s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=8, n_estimators=1000; total time=   4.3s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=8, n_estimators=1000; total time=   4.3s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=20, n_estimators=1000; total time=   2.8s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=20, n_estimators=1000; total time=   3.0s\n",
      "[CV] END max_depth=15, max_features=5, min_samples_split=20, n_estimators=1000; total time=   2.9s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=20, n_estimators=500; total time=   2.2s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=2, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=7, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=8, n_estimators=500; total time=   2.1s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=8, n_estimators=1000; total time=   2.7s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=8, n_estimators=500; total time=   2.0s\n",
      "[CV] END max_depth=15, max_features=8, min_samples_split=8, n_estimators=500; total time=   1.9s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=8, n_estimators=1000; total time=   3.0s\n",
      "[CV] END max_depth=8, max_features=7, min_samples_split=8, n_estimators=1000; total time=   2.7s\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=50; total time=   0.1s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=60; total time=   0.1s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=60; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=60; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=70; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=70; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=80; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=70; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=80; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=80; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................algorithm=SAMME.R, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=90; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=50; total time=   0.2s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=50; total time=   0.2s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=90; total time=   0.3s\n",
      "[CV] END ...................algorithm=SAMME, n_estimators=90; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=60; total time=   0.2s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=60; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................algorithm=SAMME.R, n_estimators=60; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=70; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=70; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=70; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=80; total time=   0.2s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=80; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=80; total time=   0.3s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=90; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/mdayanarshad/Desktop/Data_Science_Projects/Random Forest Classification Implementation/random_forest_venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................algorithm=SAMME.R, n_estimators=90; total time=   0.2s\n",
      "[CV] END .................algorithm=SAMME.R, n_estimators=90; total time=   0.2s\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=8, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=8, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=8, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=15, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=15, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=15, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=8, n_estimators=200; total time=   2.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=8, n_estimators=200; total time=   2.1s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=8, n_estimators=200; total time=   2.1s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=5, min_samples_split=2, n_estimators=500; total time=   4.9s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=5, min_samples_split=2, n_estimators=500; total time=   5.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=8, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=8, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=8, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.1s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=5, min_samples_split=2, n_estimators=500; total time=   5.2s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=15, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=15, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=15, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=5, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=5, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=5, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=5, min_samples_split=8, n_estimators=200; total time=   1.7s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=5, min_samples_split=8, n_estimators=200; total time=   1.8s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=20, n_estimators=200; total time=   5.2s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=5, min_samples_split=8, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=8, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=8, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=8, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=20, n_estimators=200; total time=   5.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=100; total time=   1.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=10, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=20, n_estimators=200; total time=   5.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=None, min_samples_split=20, n_estimators=200; total time=   7.2s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=None, min_samples_split=20, n_estimators=200; total time=   7.7s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=100; total time=   1.4s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=None, min_samples_split=8, n_estimators=500; total time=   9.3s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=100; total time=   1.4s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.4s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=1000; total time=  10.6s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=100; total time=   1.4s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=None, min_samples_split=8, n_estimators=500; total time=   9.1s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.3s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=  11.1s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=None, min_samples_split=20, n_estimators=200; total time=   7.2s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=  12.1s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=2, n_estimators=200; total time=   3.6s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=None, min_samples_split=8, n_estimators=500; total time=   8.6s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=8, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=8, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=500; total time=   9.1s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=200; total time=   2.1s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=1000; total time=  10.6s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=2, n_estimators=100; total time=   0.9s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=5, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=5, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=5, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=500; total time=   9.1s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=200; total time=   1.9s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=500; total time=   9.4s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=8, n_estimators=200; total time=   3.1s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=5, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=5, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=5, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=10, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=8, n_estimators=200; total time=   3.2s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=8, n_estimators=200; total time=   3.2s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=1000; total time=   8.5s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=  12.6s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=1000; total time=   9.8s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=500; total time=   9.6s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=500; total time=   9.4s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=1000; total time=   8.9s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=  10.9s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=  11.5s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=  12.5s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   9.4s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=8, n_estimators=200; total time=   3.1s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=8, n_estimators=200; total time=   3.2s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   9.2s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   9.1s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=8, n_estimators=200; total time=   3.2s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=500; total time=   9.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=1000; total time=   7.7s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=8, n_estimators=500; total time=   9.2s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=8, n_estimators=500; total time=   8.6s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=8, n_estimators=500; total time=   9.9s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=8, n_estimators=1000; total time=   9.4s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=8, n_estimators=1000; total time=   8.6s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=8, n_estimators=1000; total time=   9.6s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=500; total time=   8.1s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=500; total time=   8.3s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=None, min_samples_split=20, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=8, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=8, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=8, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=15, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=15, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=15, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=10, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=10, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=10, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=200; total time=   3.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=15, min_samples_split=8, n_estimators=500; total time=   7.7s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=500; total time=   7.5s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=None, min_samples_split=8, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=500; total time=   7.5s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=500; total time=   7.5s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=200; total time=   3.1s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=8, min_samples_split=20, n_estimators=200; total time=   3.2s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=15, min_samples_split=20, n_estimators=1000; total time=   8.9s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=15, min_samples_split=20, n_estimators=1000; total time=   9.2s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=15, min_samples_split=20, n_estimators=1000; total time=   9.8s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=15, n_estimators=500; total time=   8.6s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=15, n_estimators=500; total time=   8.1s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=15, n_estimators=500; total time=   8.0s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=8, min_samples_split=2, n_estimators=1000; total time=  12.7s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=8, min_samples_split=2, n_estimators=1000; total time=  12.2s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=8, min_samples_split=2, n_estimators=1000; total time=  12.1s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=500; total time=   5.6s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=20, n_estimators=1000; total time=  13.5s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=500; total time=   6.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=2, n_estimators=500; total time=   5.4s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=20, n_estimators=1000; total time=  14.5s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=5, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=5, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=5, min_samples_split=15, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=8, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=5, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=5, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=5, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=15, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=5, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=5, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=5, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=20, n_estimators=100; total time=   3.5s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=20, n_estimators=100; total time=   3.5s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=None, min_samples_split=20, n_estimators=100; total time=   3.5s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=None, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=8, min_samples_split=20, n_estimators=1000; total time=  12.7s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=1000; total time=  10.3s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=1000; total time=  10.1s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.4s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=100; total time=   2.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.0s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=1000; total time=   9.8s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=100; total time=   2.1s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=10, min_samples_split=2, n_estimators=200; total time=   5.5s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=10, min_samples_split=20, n_estimators=100; total time=   1.9s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=100; total time=   2.4s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=100; total time=   2.7s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=log_loss, max_depth=5, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=deviance, max_depth=None, min_samples_split=15, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=20, n_estimators=100; total time=   2.6s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=15, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=15, min_samples_split=2, n_estimators=200; total time=   7.6s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=15, min_samples_split=15, n_estimators=100; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=8, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=8, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=exponential, max_depth=8, min_samples_split=20, n_estimators=200; total time=   0.0s\n",
      "[CV] END criterion=friedman_mse, loss=exponential, max_depth=15, min_samples_split=2, n_estimators=200; total time=   7.3s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=2, n_estimators=1000; total time=   6.5s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=2, n_estimators=1000; total time=   7.5s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=mse, loss=deviance, max_depth=15, min_samples_split=2, n_estimators=1000; total time=   0.0s\n",
      "[CV] END criterion=squared_error, loss=log_loss, max_depth=15, min_samples_split=2, n_estimators=1000; total time=   7.1s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=8, min_samples_split=15, n_estimators=100; total time=   1.6s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.5s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=None, min_samples_split=2, n_estimators=200; total time=   5.2s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=8, min_samples_split=15, n_estimators=100; total time=   1.4s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=8, min_samples_split=15, n_estimators=100; total time=   1.3s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   9.2s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   9.2s\n",
      "[CV] END criterion=friedman_mse, loss=log_loss, max_depth=5, min_samples_split=20, n_estimators=1000; total time=   8.7s\n",
      "[CV] END criterion=squared_error, loss=exponential, max_depth=None, min_samples_split=2, n_estimators=200; total time=   4.4s\n",
      "---------------------------Best params for RF---------------------------\n",
      "{'n_estimators': 200, 'min_samples_split': 2, 'max_features': 5, 'max_depth': None}\n",
      "---------------------------Best params for AdaBoost---------------------------\n",
      "{'n_estimators': 70, 'algorithm': 'SAMME.R'}\n",
      "---------------------------Best params for Gradient Boost---------------------------\n",
      "{'n_estimators': 500, 'min_samples_split': 20, 'max_depth': 10, 'loss': 'exponential', 'criterion': 'squared_error'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "for name , model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model,\n",
    "                                param_distributions=params,\n",
    "                                n_iter=100,\n",
    "                                cv=3,\n",
    "                                verbose=2,\n",
    "                                n_jobs=-1)\n",
    "    \n",
    "    random.fit(X_train,y_train)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"---------------------------Best params for {model_name}---------------------------\")\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 1.0000\n",
      "- F1 score : 1.0000\n",
      "- Precision : 1.0000\n",
      "- Recall : 1.0000\n",
      "- ROC AUC Score : 1.0000\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.9192\n",
      "- F1 score : 0.9120\n",
      "- Precision : 0.9590\n",
      "- Recall : 0.6126\n",
      "- ROC AUC Score : 0.8031\n",
      "===================================\n",
      "\n",
      "\n",
      "AdaBoost Classifier\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 0.8731\n",
      "- F1 score : 0.8576\n",
      "- Precision : 0.7935\n",
      "- Recall : 0.4321\n",
      "- ROC AUC Score : 0.7032\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.8476\n",
      "- F1 score : 0.8268\n",
      "- Precision : 0.7188\n",
      "- Recall : 0.3613\n",
      "- ROC AUC Score : 0.6635\n",
      "===================================\n",
      "\n",
      "\n",
      "Gradient Boost Classifier\n",
      "-----------------------------------\n",
      "Model Performance for training set\n",
      "- Accuracy : 1.0000\n",
      "- F1 score : 1.0000\n",
      "- Precision : 1.0000\n",
      "- Recall : 1.0000\n",
      "- ROC AUC Score : 1.0000\n",
      "-----------------------------------\n",
      "Model Performance for testing set\n",
      "- Accuracy : 0.9560\n",
      "- F1 score : 0.9546\n",
      "- Precision : 0.9568\n",
      "- Recall : 0.8115\n",
      "- ROC AUC Score : 0.9013\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now training the model using the best param found through RandomizedSearchCV\n",
    "models = {\n",
    "    \"Random Forest\":RandomForestClassifier(n_estimators=500,min_samples_split=2,max_features=8,max_depth=None),\n",
    "    \"AdaBoost Classifier\":AdaBoostClassifier(n_estimators=70,algorithm='SAMME.R'),\n",
    "    \"Gradient Boost Classifier\":GradientBoostingClassifier(n_estimators=500,min_samples_split=20,max_depth=10,loss='exponential',criterion='squared_error')\n",
    "}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train,y_train) # Model Training\n",
    "\n",
    "    # Making predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Training set performance\n",
    "    model_train_accuracy = accuracy_score(y_train,y_train_pred)\n",
    "    model_train_f1 = f1_score(y_train,y_train_pred,average='weighted')\n",
    "    model_train_precision = precision_score(y_train,y_train_pred)\n",
    "    model_train_recall = recall_score(y_train,y_train_pred)\n",
    "    model_train_roc_auc_score = roc_auc_score(y_train,y_train_pred)\n",
    "\n",
    "    # Test set performance\n",
    "    model_test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "    model_test_f1 = f1_score(y_test,y_test_pred, average='weighted')\n",
    "    model_test_precision = precision_score(y_test,y_test_pred)\n",
    "    model_test_recall = recall_score(y_test,y_test_pred)\n",
    "    model_test_roc_auc_score = roc_auc_score(y_test,y_test_pred)\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Model Performance for training set')\n",
    "    print('- Accuracy : {:.4f}'.format(model_train_accuracy))\n",
    "    print('- F1 score : {:.4f}'.format(model_train_f1))\n",
    "    print('- Precision : {:.4f}'.format(model_train_precision))\n",
    "    print('- Recall : {:.4f}'.format(model_train_recall))\n",
    "    print('- ROC AUC Score : {:.4f}'.format(model_train_roc_auc_score))\n",
    "\n",
    "    print('-----------------------------------')\n",
    "    print('Model Performance for testing set')\n",
    "    print('- Accuracy : {:.4f}'.format(model_test_accuracy))\n",
    "    print('- F1 score : {:.4f}'.format(model_test_f1))\n",
    "    print('- Precision : {:.4f}'.format(model_test_precision))\n",
    "    print('- Recall : {:.4f}'.format(model_test_recall))\n",
    "    print('- ROC AUC Score : {:.4f}'.format(model_test_roc_auc_score))\n",
    "\n",
    "\n",
    "    print('='*35)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
